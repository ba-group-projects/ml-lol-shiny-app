library(shinydashboard)
library(shinyjs)

############################################################
library(caret)
library(tree)
library(randomForest)
library(dplyr)
library(rattle)
library(corrplot)
library(rpart)
library(rpart.plot)
library(stringr)

# read data
setwd("~/OneDrive/Documents/MyOversea/Cass Study/machine learning/MTP/MTP1/machine-learning-gp1")
# setwd("/Volumes/GoogleDrive-117044175360160401988/My Drive/github/machine-learning-gp1")

lol.ori <- read.csv("high_diamond_ranked_10min.csv", header = TRUE)

# summary of data
# str(lol.ori)
summary(lol.ori)

# pre-process data
sum(is.na(lol.ori)) # check missing values
lol.ori <- lol.ori[, -1]

set.seed(100)
lol.blue <- lol.ori[sample(which(lol.ori$blueWins == 1, ), 240), ]
lol.red <- lol.ori[sample(which(lol.ori$blueWins == 0, ), 240), ]
lol.sample <- rbind(lol.blue, lol.red)

# feature engineering
par(mfrow = c(1, 1))
blue.features <- lol.sample[, c(2:20)]
corrplot(cor(blue.features), tl.col = "black", diag = FALSE)
drop.blue.features <- c(
  "blueDragons", "blueHeralds", "blueKills",
  "blueDeaths", "blueAssists", "blueTotalGold",
  "blueAvgLevel", "blueTotalExperience", "blueExperienceDiff",
  "blueGoldPerMin", "blueTotalMinionsKilled"
)
blue.features <- blue.features[, !(colnames(blue.features) %in% drop.blue.features)]

red.features <- lol.sample[, -c(1, 2:20)]
corrplot(cor(blue.features, red.features), tl.col = "black", diag = TRUE)
drop.red.features <- c(
  "redFirstBlood", "redKills", "redDeaths", "redAssists",
  "redEliteMonsters", "redDragons", "redTotalGold",
  "redAvgLevel", "redTotalExperience", "redTotalMinionsKilled",
  "redGoldDiff", "redExperienceDiff", "redCSPerMin", "redGoldPerMin"
)
red.features <- red.features[, !(colnames(red.features) %in% drop.red.features)]
blueWins <- lol.sample$blueWins

lol <- cbind(blue.features, red.features, blueWins)
corrplot(cor(lol), tl.col = "black", diag = FALSE)
drop.more.features <- c(
  "blueWardsPlaced", "blueWardsDestroyed", "blueTowersDestroyed",
  "redWardsPlaced", "redWardsDestroyed", "redTowersDestroyed"
)
lol <- lol[, !(colnames(lol) %in% drop.more.features)]

# modify classification column
lol$blueWins[lol$blueWins == 1] <- "Blue"
lol$blueWins[lol$blueWins == 0] <- "Red"
lol$blueWins <- factor(lol$blueWins)

set.seed(100)
# random split to training and test set
# train.index = createDataPartition(lol$blueWins, p = 0.6, list = FALSE)
train.index <- createDataPartition(lol$blueWins, p = 0.7, list = FALSE)
train <- lol[train.index, ]
test <- lol[-train.index, ]

############################################################
### CUSTOM FUNCTIONS ###
############################################################

train.test.split <- function(data, p) {
  train.index <- createDataPartition(data$blueWins, p = p, list = FALSE)
  train <- data[train.index, ]
  test <- data[-train.index, ]

  return(list(train, test))
}

###################################
### >>> DECISION TREE <<< ###
###################################

createTree <- function(train_data, min_split = NULL, min_bucket = NULL, max_depth = NULL, optimise = TRUE) {
  # Takes a list of model variables (strings), a minimum split parameter
  # (int), a minimum bucket size parameter (int), and a maximum tree depth
  # parameter (int) as inputs and then returns an rpart classification tree
  # created with those parameters using Gini-indexes without any pruning.
  # create an rpart compatible formula for the model from the chosen vars
  # f <- paste("blueWins ~ ", paste(model_vars, collapse = " + "))
  # rpart is performs the split calculations and returns the tree
  if (optimise == TRUE) {
    set.seed(100)
    tree <- rpart(
      # as.formula(f),
      "blueWins ~ .",
      method = "class", # sets it up as a classification problem
      data = train_data,
      parms = list(split = "gini")
    )
  } else {
    set.seed(100)
    tree <- rpart(
      # as.formula(f),
      "blueWins ~ .",
      method = "class", # sets it up as a classification problem
      data = train_data,
      parms = list(split = "gini"), # ensures rpart uses gini indexes
      minsplit = min_split,
      minbucket = min_bucket,
      maxdepth = max_depth,
      cp = 0 # complexity parameter, at zero prevents pruning on branches
    )
  }

  return(tree)
}

evaluteTree <- function(tree) {
  # Takes a tree generated by rpart and a filename (string) as input and
  # then predicts the labels of the data in that file using the tree. It
  # returns a dataframe with two bool (0,1) columns: prediction and truth.
  trainX <- train[, -(length(train) + 1)]
  trainY <- train[, length(train)]
  testX <- test[, -(length(test) + 1)]
  testY <- test[, length(test)]
  pred.y.Train <- predict(tree, trainX, type = "class")
  pred.y.Test <- predict(tree, testX, type = "class")
  accuracyTrain <- mean(pred.y.Train == trainY)
  accuracyTest <- mean(pred.y.Test == testY)
  return(list(train.accuracy = accuracyTrain, test.accuracy = accuracyTest))
}

useTree <- function(tree, data) {
  # Takes a tree generated by rpart and a filename (string) as input and
  # then predicts the labels of the data in that file using the tree. It
  # returns a dataframe with two bool (0,1) columns: prediction and truth.

  prediction <- predict(tree, data, type = "class")
  results <- as.data.frame(prediction)
  results$truth <- data$blueWins

  return(results)
}


get.dt.features <- function(tree) {
  rpart.rules <- data.frame(labels(tree))[1][-1, ]

  imp.features <- c()
  for (feature in str_extract_all(rpart.rules, "\\w*")) {
    # print(feature[1])
    imp.features <- rbind(imp.features, feature[1])
  }
  output <- unique(imp.features)
  return(paste(output))
}

###################################
### >>> RANDOM FOREST <<< ###
###################################
train.rf.tuned <- function(ntree, train, test) {
  fitcontrol.rf <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
  model <- train(train[, -ncol(train)], train[, ncol(train)],
    method = "rf",
    metric = "Accuracy", tuneLength = 5,
    trControl = fitcontrol.rf, ntree = ntree
  )

  pred.train <- predict(model, newdata = train[, -ncol(train)])
  pred.train.acc <- round(mean(pred.train == train[, ncol(train)]) * 100, 1)

  pred.test <- predict(model, newdata = test[, -ncol(test)])
  pred.test.acc <- round(mean(pred.test == test[, ncol(test)]) * 100, 1)

  return(list(model, pred.train.acc, pred.test.acc))
}

predict.rf.tuned <- function(model, user.input) {
  prediction <- predict(model, user.input)
  return(prediction)
}
###################################
### >>> PERFORMANCE MEASURE <<< ###
###################################

calcScores <- function(results) {
  # Takes a results dataframe as input and then calculates scores for
  # accuracy, true negative rate, and true positive rate. It returns a
  # list of formatted strings detailing these results.

  results <- table(results)
  # calculate the scores on which we'll judge our model to 2 decimal places
  accuracy <- round(100 * (results[1] + results[4]) / sum(results), 2)
  true_neg <- round(100 * results[1] / sum(results[1, ]), 2)
  true_pos <- round(100 * results[4] / sum(results[2, ]), 2)

  # the collapse argument removes the spacing which would otherwise be there
  return(list(
    paste(c("Overall Accuracy: ", accuracy, "%"), collapse = ""),
    paste(c("True Positive Rate: ", true_pos, "%"), collapse = ""),
    paste(c("True Negative Rate: ", true_neg, "%"), collapse = "")
  ))
}

resultsTable <- function(results) {
  # Takes a results dataframe as input and then reconstructs and returns
  # a dataframe which has a similar layout to the command line interface
  # output of R's table(...) function.

  data <- table(results)
  Outcomes <- c("Predicted Blue Win", "Predicted Red Win", "Total")
  # reconstruct the columns of R's table(...) CLI display
  c1 <- c(data[, 1], sum(data[, 1])) # data[, 1] is a length 2 vector
  c2 <- c(data[, 2], sum(data[, 1])) # data[, 2] is a length 2 vector
  c3 <- c(sum(data[, 1]), sum(data[2, ]), sum(data))

  # turn these columns back into a dataframe but with proper headers
  output <- data.frame(Outcomes)
  output$"Actually Blue Win" <- c1
  output$"Actually Red Win" <- c2
  output$"Total" <- c3

  return(output)
}

############################################################
### SHINY UI COMPONENTS ###
############################################################

# header

headerbar <- dashboardHeader(
  title = span(img(src = "logo.png", height = 40), "LOL Diamond Rank Analytics"), # TODO solve broken picture
  # titleWidth = 300,
  tags$li(class = "dropdown", tags$style(".skin-blue .main-header .navbar {background-color: #111111;}"))
)

# sidebar
sidebar <- dashboardSidebar(
  sidebarMenu(
    menuItem("Introduction",
      tabName = "introduction", icon = icon("dashboard"),
      menuItem("Classification model", tabName = "classificationModel"),
      menuItem("Dataset", tabName = "dataset")
    ),
    # menuItem("Model Selection",
    #   icon = icon("tree"), tabName = "modelSelection",

    menuItem("Decision Tree",
      tabName = "decisionTree", icon = icon("tree"),
      menuItem("Train", tabName = "decisionTreeTrain"),
      menuItem("Predict", tabName = "decisionTreePredict")
    ),
    menuItem("Random Forest",
      tabName = "randomForest", icon = icon("cubes"),
      menuItem("Train", tabName = "randomForestTrain"),
      menuItem("Predict", tabName = "randomForestPredict")
    ),
    menuItem("FAQ",
      tabName = "faq", icon = icon("question-circle")
    )
    # menuItem("Prediction", tabName = "prediction", icon = icon("clock")),
    # menuItem("Lexicon", tabName = "lexicon", icon = icon("table")),
    # menuItem("Introduction Video", tabName = "introductionVideo", icon = icon("video"))
  )
)


# dashboardBody
dashboardContent <-
  dashboardBody(
    # # Boxes need to be put in a row (or column)
    # fluidRow(
    #   box(plotOutput("plot1", height = 600)),

    #   box(
    #     title = "Controls",
    #     sliderInput("slider", "Number of observations:", 1, 100, 50)
    #   )
    # ),
    tabItems(
      tabItem(tabName = "classificationModel", fluidRow(fluidRow(HTML('<!DOCTYPE html>
<html>
<div style="margin-top:-50px">
<img src="forest3.png" style="max-height:20px overflow:hidden">
</div>
<div style="margin-left:10%;margin-right:15%;margin-top:4%;background:white;padding:40px ">
<h2>
    Introduction:
</h2>
<br>
<p style="font-size:16px">
    <p style="font-size:16px">The aim of this app is to explain how decision trees and random forest works in classification problems. We aim to showcase this by creating a classification model to predict the winner of League of Legends, an online video game. Users will be able to play around with the model’s hyperparameters to see its effect on the model performance.

    </p>
    <br>


    <h3>What Is A Decision tree</h3>
    <br>
    <br>
    <p style="font-size:16px">Decision trees perform a classification on data in a hierarchical structure by observing the key differentiating features on the classification data. The path is segregated into binary “yes”, “no” decisions at each level leading to another question and ultimately the result. Each question is regarded as a node, with the split becoming “branches” and the “leaves” representing the end of each point.  </p>
    <br>
    <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/7VeUPuFGJHk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br>
    <br>
    <h3>What Is A Random Forest</h3>
    <br>
    <br>
    <p style="font-size:16px">
    Random forests is a modified approach to bagging (bootstrap sampling that averages the aggregate of individual models), in which an algorithm inspects a random subset of features in the data at each split in the learning process, rather than all features seen in bagging. This is done to avoid correlation between the trees. Using multiple bootstrapped samples of the original dataset reduces variance, resulting in lower overfitting.
    </p>
    <br>
    <br>
    <img src="randomforest.gif" width=500>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/J4Wdy0Wc_xQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <div align="center">
    
    </div>
    <br>

    
</br>
</div>
</html>')))),
      tabItem(tabName = "dataset", fluidRow(fluidRow(HTML('<!DOCTYPE html>
<html>
<div style="margin-top:-50px">
<img src="lolbanner3.png" style="max-height:100px;overflow:hidden;width:100%">
</div>
<div style="margin-left:10%;margin-right:15%;margin-top:4%;background:white;padding:20px ">
<h2>
    Dataset:
</h2>
<br>
<p style="font-size:16px">
    <p style="font-size:16px">The dataset comes from the first 10 minutes of 10,000 ranked games from League of Legends, a MOBA (multiplayer online battle arena) where 2 teams (blue and red) play against each other. Players in these ranked games are roughly the same level. The dataset contains 38 features (19 per team) containing information relating to score per minute (SPM), enemies killed, and rank. For this app, we will start with 37 predictors and the classification will be if the blue team wins. 
    </p>
    <br>
  
</br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/0uyLRPmmYPk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
<br>
<br>
<br>
<br>

</div>
 
</html>')))),

      tabItem(
        tabName = "decisionTreeTrain",
        fluidRow(
          box(
            fluidRow(
              plotOutput("decisionTreeTrainPlot", height = 600)
            ),
            fluidRow(
              height = 400,
              column(
                6,
                # plotOutput("decisionTreeTrainPlot1", height = 600))
                h3("Training Results"),
                helpText(
                  "Training results measure how good the model was when it ran on the training data set. Using the slider will determine the percentage of the dataset that will be trained. Positive results reflect the blue team winning. "
                ),
                # training accuracy, true positive, and true negative
                tagAppendAttributes(
                  textOutput("training_scores"),
                  # allow linebreaks between scores, larger font here
                  style = "white-space: pre-wrap; font-size: 17px;"
                ),
                br(),
                # training results table matches layout from presentation
                tableOutput("training_table")
              ),
              column(
                6,
                h3("Test Results"),
                helpText(
                  "These are the measures of how good your model was when it was ran on the test data set. The test result percentage is the difference of the training slider."
                ),
                # test accuracy, true positive, and true negative
                tagAppendAttributes(
                  textOutput("test_scores"),
                  # allow linebreaks between scores, larger font here
                  style = "white-space: pre-wrap; font-size: 17px;"
                ),
                br(),
                # training results table matches layout from presentation
                tableOutput("test_table")
              )
            )
          ),
          box(
            h3("Decision Tree"),
            helpText(
              "These controls are for setting the hyperparameter values",
              "which partly control the structure of the decision tree.",
              "The default values we've put in should create a fairly safe",
              "tree but try changing them if you're feeling adventurous."
            ),
            br(),
            helpText(
              ""
            ),
            h4("Pick a Decision Tree"),
            helpText(
              ""
            ),
            # radioButtons("custOpt", "", c("Customize", "Optimize"),inline = TRUE,width='100%'), #TODO change the space
            radioButtons("custOpt", "", c("Optimized Tree", "Grow your own Tree!"), inline = TRUE, width = "100%"),
            br(),
            actionButton(
              inputId = "trainModel",
              label = "Train Model",
              class = "btn-primary", # "btn-danger" # makes it blue!
              style = "color: #fff"
            ),
            br(),
            br(),
            h4("Split Size (%)"),

            sliderInput(
              inputId = "splitSize",
              label = "", # label given in outer code
              min = 0, # two is the smallest that could be split
              max = 100, # chosen to not make the models too wild
              value = 70 # defaults to not having an artifical minimum
            ),
            br(),
            conditionalPanel(
              condition = "input.custOpt == 'Grow your own Tree!'",
              # box(
              h4("Minimum Split"),
            helpText("Specifies the minimum number of samples required to split an internal node. 
            If the sample size of the node is smaller than this value, the node will not split any further. 
            This will become the terminal node (leaf) of the tree."),
              sliderInput(
                inputId = "minSplit",
                label = NULL, # label given in outer code
                min = 2, # two is the smallest that could be split
                max = 10, # chosen to not make the models too wild
                value = 3 # defaults to not having an artifical minimum
              ),
              br(),
              h4("Minimum Bucket Size"),
              helpText(
               "The smallest number of samples allowed in a leaf node. The split will not happen if the number of samples in the node is less than the specified minimum bucket size.
"
              ),
              sliderInput(
                inputId = "minBucket",
                label = NULL, # label given in outer code
                min = 1, # can't have buckets of size zero
                max = 30, # rpart default is minbucket = 3*minsplit
                value = 4 # defaults to not having an artifical minimum
              ),
              br(),
              h4("Maximum Tree Depth"),
              helpText(
                "Control the maximum depth that the decision tree can reach. The deeper the tree, the more splits it can achieve, capturing more information about the dataset.
"
              ),
              sliderInput(
                inputId = "maxDepth",
                label = NULL, # label given in outer code
                min = 2, # a min of 2 allows for at least one split
                max = 5, # rpart can't do 31+ depth on 32-bit machines
                value = 3 # chosen to not make the default too wild
              )
              # )
            )
          )
        )
      ),
      tabItem(
        tabName = "randomForest",
        fluidRow(
          box(plotOutput("randomForestPlot", height = 1000))
        )
      ),
      # TODO number of trees; randomize
      ##############################
      # prediction interaction part
      ##############################
      tabItem(
        tabName = "decisionTreePredict",
        fluidRow(
          box(plotOutput("decisionTreeTrainPlot_", height = 600)),
          box(
            h3("Predict which team would win!"),
            span(textOutput("impFeatures"), style = "color:#fff"),
            actionButton(
              inputId = "treePredict",
              label = "Predict Winner",
              class = "btn-primary", # "btn-danger" # makes it blue!
              style = "color: #fff"
            ),
            br(),
            br(),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueFirstBlood') > -1",
              sliderInput("blueFirstBlood", "Blue First Blood", 0, 1, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueGoldDiff') > -1",
              sliderInput("blueGoldDiff", "Blue Gold Diff", -10000, 10000, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueCSPerMin') > -1",
              sliderInput("blueCSPerMin", "Blue CS Per Min", 0, 100, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueEliteMonsters') > -1",
              sliderInput("blueEliteMonsters", "Blue Elite Monsters", 0, 2, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueTotalJungleMinionsKilled') > -1",
              sliderInput("blueTotalJungleMinionsKilled", "Blue Total Jungle Minions Killed", -10000, 10000, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('redTotalJungleMinionsKilled') > -1",
              sliderInput("redTotalJungleMinionsKilled", "Red Total Jungle Minions Killed", 0, 200, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('redHeralds') > -1",
              sliderInput("redHeralds", "Red Heralds", 0, 1, 0)
            )

            # radioButtons("firstBlood", "First Blood", c("Blue", "Red")),
            # radioButtons("herald", "Herald", c("Blue", "Red")),
            # sliderInput("blueWardsPlaced", "Blue wards placed", 0, 60, 20),
            # sliderInput("blueWardsDestroyed", "Red wards placed", 0, 120, 20),
            # sliderInput("blueELiteMonsters", "Blue Elite Monsters", 0, 2, 1),
            # sliderInput("blueTowersDestroyed", "Blue Towers Destroyed", 0, 1, 1),
            # sliderInput("blueTotalJungleMinionsKilled", "Blue Total Jungle Minions Killed", 0, 80, 20),
            # sliderInput("blueTotalGold", "Blue Total Gold", -10000, 10000, 0),
            # sliderInput("blueMinionKillsPerMin", "Blue Minion Kills Per Min", 10, 30, 20),
            # sliderInput("redWardsPlaced", "Red wards placed", 0, 60, 20),
            # sliderInput("redWardsPlaced", "Red wards placed", 0, 60, 20),
            # sliderInput("redTowersDestroyed", "Red Towers Destroyed", 0, 1, 1),
            # sliderInput("redTotalJungleMinionsKilled", "Red Total Jungle Minions Killed", 0, 80, 20),
            # sliderInput("redTotalGold", "Red Total Gold", -10000, 10000, 0)
          )
        )
      ),
      #       tabItem(
      #         tabName = "lexicon",
      #         HTML('<!DOCTYPE html>
      # <html>
      # <head>
      #   <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
      #   <script type="application/shiny-singletons"></script>
      #   <script type="application/html-dependencies">jquery[3.6.0];shiny-css[1.7.1];shiny-javascript[1.7.1]</script>
      #   <head>
      #     <link href="shared/shiny.min.css" rel="stylesheet" />
      #     <script src="shared/shiny.min.js"></script>
      #     </head>
      #     <body>
      #         <h1>Lexicon</h1>
      #         <table>
      #         <tr>
      #             <th>Name</th>
      #             <th>Explanation</th>
      #           </tr>
      #           <tr>
      #             <td>Alfreds Futterkiste</td>
      #             <td>Maria Anders</td>
      #           </tr>
      #         </table>
      #         </body>>
      # </html>')
      #       ),
      # tabItem(
      #   tabName = "introductionVideo",
      #   HTML('<iframe width="560" height="315" src="https://www.youtube.com/embed/0uyLRPmmYPk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>')
      # ),
        tabItem(tabName = "dataset", fluidRow(fluidRow(HTML('
        <!DOCTYPE html>
          <html>
          <div style="margin-top:-50px">
          <img src="lolbanner3.png" style="max-height:100px;overflow:hidden;width:100%">
          </div>
          <div style="margin-left:10%;margin-right:15%;margin-top:4%;background:white;padding:20px ">
          <h2>
              Dataset:
          </h2>
          <br>
          <p style="font-size:16px">
              <p style="font-size:16px">The dataset comes from the first 10 minutes of 10,000 ranked games from League of Legends, a MOBA (multiplayer online battle arena) where 2 teams (blue and red) play against each other. Players in these ranked games are roughly the same level. The dataset contains 38 features (19 per team) containing information relating to score per minute (SPM), enemies killed, and rank. For this app, we will start with 37 predictors and the classification will be if the blue team wins. 
              </p>
              <br>
            
          </br>
          <iframe width="560" height="315" src="https://www.youtube.com/embed/0uyLRPmmYPk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <br>
          <br>
          <br>
          <br>
          <br>

          </div>
          
          </html>'))))

    ),

    tags$head(tags$style(HTML("
            /* logo */
            .skin-blue .main-header .logo {
                                  background-color: #111111;
                                  };
    #         /* whole page */
    #         .box {margin-top: 2px;margin-left: 0px; margin-right: 0px; margin-bottom:2px;padding:-10px};
    # div {padding: 0 !important;}
                                  ")))
  )


############################################################
### SHINY APP ###
############################################################

ui <- dashboardPage(
  # useShinyjs(),
  # extendShinyjs(text = jsToggleFS),
  headerbar,
  sidebar,
  dashboardContent
)

server <- function(input, output, session) {
  set.seed(122)
  # histdata <- rnorm(500)
  # output$randomForestPlot <- renderPlot(hist(histdata, plot = FALSE), "plot1")
  # decisionTree <- createTree(train, observe(input$minSplit), observe(input$minBucket), observe(input$maxDepth))
  decisionTree <- eventReactive(
    eventExpr = input$trainModel,
    {
      train.test <- train.test.split(lol, input$splitSize / 100)
      train <- train.test[[1]]
      test <- train.test[[2]]

      if (input$custOpt == "Optimized Tree") {
        valueExpr <- createTree(train)
      } else {
        valueExpr <- createTree(train, input$minSplit, input$minBucket, input$maxDepth, optimise = FALSE)
        # print(input$custOpt)
        # print(valueExpr)
      }
    }
  )
  output$decisionTreeTrainPlot_ <- renderPlot(
    rpart.plot(decisionTree(), box.palette = "BuRd", roundint = FALSE)
  )
  output$decisionTreeTrainPlot <- renderPlot(
    rpart.plot(decisionTree(), box.palette = "BuRd", roundint = FALSE)
  )

  output$impFeatures <- renderText(get.dt.features(decisionTree()))

  ##############################
  #### eventReactive############
  ##############################

  # regenerate training results every time createModel is pressed
  training_results <- eventReactive(
    eventExpr = input$trainModel,
    valueExpr = useTree(decisionTree(), train)
  )
  test_results <- eventReactive(
    eventExpr = input$trainModel,
    valueExpr = useTree(decisionTree(), test)
  )

  ##############################
  #### observeEvent############
  ##############################

  # observeEvent(input$addLine, {
  #   new_id <- paste("row", input$addLine, sep = "_")
  #   insertUI(
  #     selector = "#placeholder",
  #     where = "beforeBegin",
  #     ui = sliderInput("blueWardsDestroyed", "Red wards placed", 0, 120, 20))}
  #   )

  # # obvervation for customizer and optimizer
  # observeEvent(input$custOpt, {
  #   if (input$custOpt == 'Customizer'){
  #     shinyjs::show(id = "customizer-optimizer")
  #   }else {
  #     shinyjs::hidden(id = "customizer-optimizer")
  #   }
  # })

  output$training_scores <- renderText(
    paste(calcScores(training_results()), collapse = "\n")
  )
  output$test_scores <- renderText(
    paste(calcScores(test_results()), collapse = "\n")
  )
  output$training_table <- renderTable(
    resultsTable(training_results()),
    align = "lccc", # left-align first column, centre rest
    striped = TRUE
  )
  output$test_table <- renderTable(
    resultsTable(test_results()),
    align = "lccc", # left-align first column, centre rest
    striped = TRUE
  )
  # height = 1000,
  # width = 1000,
  # xlab = "",
  # ylab = "",
  # main = "Decision Tree"
}

shinyApp(ui, server)