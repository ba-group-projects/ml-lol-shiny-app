library(shinydashboard)
library(shinyjs)

############################################################
library(caret)
library(tree)
library(randomForest)
library(dplyr)
library(rattle)
library(corrplot)
library(rpart)
library(rpart.plot)
library(stringr)

# read data
# setwd("~/OneDrive/Documents/MyOversea/Cass Study/machine learning/MTP/MTP1/machine-learning-gp1")
setwd("/Volumes/GoogleDrive-117044175360160401988/My Drive/github/machine-learning-gp1")

lol.ori <- read.csv("high_diamond_ranked_10min.csv", header = TRUE)

# summary of data
# str(lol.ori)
summary(lol.ori)

# pre-process data
sum(is.na(lol.ori)) # check missing values
lol.ori <- lol.ori[, -1]

set.seed(100)
lol.blue <- lol.ori[sample(which(lol.ori$blueWins == 1, ), 240), ]
lol.red <- lol.ori[sample(which(lol.ori$blueWins == 0, ), 240), ]
lol.sample <- rbind(lol.blue, lol.red)

# feature engineering
par(mfrow = c(1, 1))
blue.features <- lol.sample[, c(2:20)]
corrplot(cor(blue.features), tl.col = "black", diag = FALSE)
drop.blue.features <- c(
  "blueDragons", "blueHeralds", "blueKills",
  "blueDeaths", "blueAssists", "blueTotalGold",
  "blueAvgLevel", "blueTotalExperience", "blueExperienceDiff",
  "blueGoldPerMin", "blueTotalMinionsKilled"
)
blue.features <- blue.features[, !(colnames(blue.features) %in% drop.blue.features)]

red.features <- lol.sample[, -c(1, 2:20)]
corrplot(cor(blue.features, red.features), tl.col = "black", diag = TRUE)
drop.red.features <- c(
  "redFirstBlood", "redKills", "redDeaths", "redAssists",
  "redEliteMonsters", "redDragons", "redTotalGold",
  "redAvgLevel", "redTotalExperience", "redTotalMinionsKilled",
  "redGoldDiff", "redExperienceDiff", "redCSPerMin", "redGoldPerMin"
)
red.features <- red.features[, !(colnames(red.features) %in% drop.red.features)]
blueWins <- lol.sample$blueWins

lol <- cbind(blue.features, red.features, blueWins)
corrplot(cor(lol), tl.col = "black", diag = FALSE)
drop.more.features <- c(
  "blueWardsPlaced", "blueWardsDestroyed", "blueTowersDestroyed",
  "redWardsPlaced", "redWardsDestroyed", "redTowersDestroyed"
)
lol <- lol[, !(colnames(lol) %in% drop.more.features)]

# modify classification column
lol$blueWins[lol$blueWins == 1] <- "Blue"
lol$blueWins[lol$blueWins == 0] <- "Red"
lol$blueWins <- factor(lol$blueWins)

set.seed(100)
# random split to training and test set
# train.index = createDataPartition(lol$blueWins, p = 0.6, list = FALSE)
train.index <- createDataPartition(lol$blueWins, p = 0.7, list = FALSE)
train <- lol[train.index, ]
test <- lol[-train.index, ]

############################################################
### CUSTOM FUNCTIONS ###
############################################################

train.test.split <- function(data, p) {
  set.seed(100)
  train.index <- createDataPartition(data$blueWins, p = p, list = FALSE)
  train <- data[train.index, ]
  test <- data[-train.index, ]

  return(list(train, test))
}

    ###################################
    ### >>> DECISION TREE <<< ###
    ###################################

createTree <- function(train_data, min_split = NULL, min_bucket = NULL, max_depth = NULL, optimise = TRUE) {
  # Takes a list of model variables (strings), a minimum split parameter
  # (int), a minimum bucket size parameter (int), and a maximum tree depth
  # parameter (int) as inputs and then returns an rpart classification tree
  # created with those parameters using Gini-indexes without any pruning.
  # create an rpart compatible formula for the model from the chosen vars
  # f <- paste("blueWins ~ ", paste(model_vars, collapse = " + "))
  # rpart is performs the split calculations and returns the tree
  if (optimise == TRUE) {
    set.seed(100)
    tree <- rpart(
      # as.formula(f),
      "blueWins ~ .",
      method = "class", # sets it up as a classification problem
      data = train_data,
      parms = list(split = "gini")
    )
  } else {
    set.seed(100)
    tree <- rpart(
      # as.formula(f),
      "blueWins ~ .",
      method = "class", # sets it up as a classification problem
      data = train_data,
      parms = list(split = "gini"), # ensures rpart uses gini indexes
      minsplit = min_split,
      minbucket = min_bucket,
      maxdepth = max_depth,
      cp = 0 # complexity parameter, at zero prevents pruning on branches
    )
  }

  return(tree)
}

evaluteTree <- function(tree) {
  # Takes a tree generated by rpart and a filename (string) as input and
  # then predicts the labels of the data in that file using the tree. It
  # returns a dataframe with two bool (0,1) columns: prediction and truth.
  trainX <- train[, -(length(train) + 1)]
  trainY <- train[, length(train)]
  testX <- test[, -(length(test) + 1)]
  testY <- test[, length(test)]
  pred.y.Train <- predict(tree, trainX, type = "class")
  pred.y.Test <- predict(tree, testX, type = "class")
  accuracyTrain <- mean(pred.y.Train == trainY)
  accuracyTest <- mean(pred.y.Test == testY)
  return(list(train.accuracy = accuracyTrain, test.accuracy = accuracyTest))
}

useTree <- function(tree, data) {
  # Takes a tree generated by rpart and a filename (string) as input and
  # then predicts the labels of the data in that file using the tree. It
  # returns a dataframe with two bool (0,1) columns: prediction and truth.

  prediction <- predict(tree, data, type = "class")
  results <- as.data.frame(prediction)
  results$truth <- data$blueWins

  return(results)
}

get.dt.features<-function(tree){
  rpart.rules = data.frame(labels(tree))[1][-1,]
  
  imp.features = c()
  for (feature in str_extract_all(rpart.rules, "\\w*")) {
    # print(feature[1])
    imp.features = rbind(imp.features, feature[1])
  }
  output = unique(imp.features)
  return(paste(output))
}

    ###################################
    ### >>> RANDOM FOREST <<< ###
    ###################################
train.rf.tuned <- function(ntree, train, test) {
  fitcontrol.rf <- trainControl(method = "repeatedcv", number = 10, repeats = 2)
  model <- train(train[, -ncol(train)], train[, ncol(train)],
    method = "rf",
    metric = "Accuracy", tuneLength = 5,
    trControl = fitcontrol.rf, ntree = ntree
  )

  pred.train <- predict(model, newdata = train[, -ncol(train)])
  pred.train.acc <- round(mean(pred.train == train[, ncol(train)]) * 100, 1)

  pred.test <- predict(model, newdata = test[, -ncol(test)])
  pred.test.acc <- round(mean(pred.test == test[, ncol(test)]) * 100, 1)

  return(list(model, pred.train.acc, pred.test.acc))
}

predict.winner <- function(model, user.input, model.type) {
  
  if (model.type == "dt") {
      prediction = predict(model, user.input)
      prediction = colnames(data.frame(prediction))[max.col(data.frame(prediction))]
  } else {
      prediction = predict(model, user.input)
  }
  
  return(prediction)
}

    ###################################
    ### >>> PERFORMANCE MEASURE <<< ###
    ###################################

calcScores <- function(results) {
  # Takes a results dataframe as input and then calculates scores for
  # accuracy, true negative rate, and true positive rate. It returns a
  # list of formatted strings detailing these results.

  results <- table(results)
  # calculate the scores on which we'll judge our model to 2 decimal places
  accuracy <- round(100 * (results[1] + results[4]) / sum(results), 2)
  true_neg <- round(100 * results[1] / sum(results[1, ]), 2)
  true_pos <- round(100 * results[4] / sum(results[2, ]), 2)

  # the collapse argument removes the spacing which would otherwise be there
  return(list(
    paste(c("Overall Accuracy: ", accuracy, "%"), collapse = ""),
    paste(c("True Positive Rate: ", true_pos, "%"), collapse = ""),
    paste(c("True Negative Rate: ", true_neg, "%"), collapse = "")
  ))
}

resultsTable <- function(results) {
  # Takes a results dataframe as input and then reconstructs and returns
  # a dataframe which has a similar layout to the command line interface
  # output of R's table(...) function.

  data <- table(results)
  Outcomes <- c("Predicted Blue Win", "Predicted Red Win", "Total")
  # reconstruct the columns of R's table(...) CLI display
  c1 <- c(data[, 1], sum(data[, 1])) # data[, 1] is a length 2 vector
  c2 <- c(data[, 2], sum(data[, 1])) # data[, 2] is a length 2 vector
  c3 <- c(sum(data[, 1]), sum(data[2, ]), sum(data))

  # turn these columns back into a dataframe but with proper headers
  output <- data.frame(Outcomes)
  output$"Actually Blue Win" <- c1
  output$"Actually Red Win" <- c2
  output$"Total" <- c3

  return(output)
}

############################################################
### SHINY UI COMPONENTS ###
############################################################

# header

headerbar <- dashboardHeader(
  title = span(img(src = "logo.png", height = 40), "LOL Diamond Rank Analytics"), # TODO solve broken picture
  # titleWidth = 300,
  tags$li(class = "dropdown", tags$style(".skin-blue .main-header .navbar {background-color: #111111;}"))
)

# sidebar
sidebar <- dashboardSidebar(
  sidebarMenu(
    menuItem("Introduction", tabName = "introduction", icon = icon("dashboard")),
    # menuItem("Model Selection",
    #   icon = icon("tree"), tabName = "modelSelection",

    menuItem("Decision Tree",
      tabName = "decisionTree", icon = icon("tree"),
      menuItem("Train", tabName = "decisionTreeTrain"),
      menuItem("Predict", tabName = "decisionTreePredict")
    ),
    menuItem("Random Forest",
      tabName = "randomForest", icon = icon("cubes"),
      menuItem("Train", tabName = "randomForestTrain"),
      menuItem("Predict", tabName = "randomForestPredict")
    )
    # menuItem("Prediction", tabName = "prediction", icon = icon("clock")),
    # menuItem("Lexicon", tabName = "lexicon", icon = icon("table")),
    # menuItem("Introduction Video", tabName = "introductionVideo", icon = icon("video"))
  )
)


# dashboardBody
dashboardContent <-
  dashboardBody(
    # # Boxes need to be put in a row (or column)
    # fluidRow(
    #   box(plotOutput("plot1", height = 600)),

    #   box(
    #     title = "Controls",
    #     sliderInput("slider", "Number of observations:", 1, 100, 50)
    #   )
    # ),
    tabItems(
      tabItem(tabName = "introduction", fluidRow(fluidRow(HTML('<!DOCTYPE html>
<html>  

<div style="margin: 10%">
<h2>
    Introduction:
</h2>
<br>
<p style="font-size:16px">
    <p style="font-size:16px">The aim of this app is to explain how decision trees and random forest works in classification problems. We aim to showcase this by creating a classification model to predict the winner of League of Legends, an online video game. Users will be able to play around with the model’s hyperparameters to see its effect on the model performance.
    
    </p>
    <br>
    
    
    <h3>What Is A Decision tree</h3>
    <br>
    <br>
    <p style="font-size:16px">Decision trees perform a classification on data in a hierarchical structure by observing the key differentiating features on the classification data. The path is segregated into binary “yes”, “no” decisions at each level leading to another question and ultimately the result. Each question is regarded as a node, with the split becoming “branches” and the “leaves” representing the end of each point.  </p>
    <br>
    <div align="center">
    <img src="randomforest.gif" width=500>
    </div>
        <br>
    <h3>What Is A Random Forest</h3>
    <br>
    <br>
    <p style="font-size:16px">
    Random forests is a modified approach to bagging (bootstrap sampling that averages the aggregate of individual models), in which an algorithm inspects a random subset of features in the data at each split in the learning process, rather than all features seen in bagging. This is done to avoid correlation between the trees. Using multiple bootstrapped samples of the original dataset reduces variance, resulting in lower overfitting. 
    </p>
</br>
</div>
</html>')))),
      # tabItem(tabName = "decisionTreeTrain"),
      tabItem(
        tabName = "decisionTreeTrain",
        fluidRow(
          box(
            fluidRow(
              plotOutput("decisionTreeTrainPlot", height = 600)
            ),
            fluidRow(
              column(
                6,
                # plotOutput("decisionTreeTrainPlot1", height = 600))
                h2("Training Results"),
                helpText(
                  "These are the measures of how good your model was",
                  "when it was ran on the training data set. Recall from",
                  "the lecture how we calculated each of these. Would",
                  "you prefer a false positive or false negative in the",
                  "context of spam detection?"
                ),
                # training accuracy, true positive, and true negative
                tagAppendAttributes(
                  textOutput("dt_train_scores"),
                  # allow linebreaks between scores, larger font here
                  style = "white-space: pre-wrap; font-size: 17px;"
                ),
                br(),
                # training results table matches layout from presentation
                tableOutput("dt_train_table")
              ),
              column(
                6,
                h2("Test Results"),
                helpText(
                  "These are the measures of how good your model was",
                  "when it was ran on the test data set. Recall what",
                  "was said in lectures about how we interpret the",
                  "differences between measures these and the measures",
                  "from the training data."
                ),
                # test accuracy, true positive, and true negative
                tagAppendAttributes(
                  textOutput("dt_test_scores"),
                  # allow linebreaks between scores, larger font here
                  style = "white-space: pre-wrap; font-size: 17px;"
                ),
                br(),
                # training results table matches layout from presentation
                tableOutput("dt_test_table")
              )
            )
          ),
          box(
            h3("Decision Tree"),
            helpText(
              "These controls are for setting the hyperparameter values",
              "which partly control the structure of the decision tree.",
              "The default values we've put in should create a fairly safe",
              "tree but try changing them if you're feeling adventurous."
            ),
            br(),
            helpText(
              ""
            ),
            h4("Pick a Decision Tree"),
            helpText(
              ""
            ),
            # radioButtons("custOpt", "", c("Customize", "Optimize"),inline = TRUE,width='100%'), #TODO change the space
            radioButtons("custOpt", "", c("Optimized Tree", "Grow your own Tree!"), inline = TRUE, width = "100%"),
            br(),
            actionButton(
              inputId = "trainDtModel",
              label = "Train Model",
              class = "btn-primary", # "btn-danger" # makes it blue!
              style = "color: #fff"
            ),
            br(),
            br(),
            h4("Split Size"),
            sliderInput(
              inputId = "splitSize",
              label = "%", # label given in outer code
              min = 0, # two is the smallest that could be split
              max = 100, # chosen to not make the models too wild
              value = 70 # defaults to not having an artifical minimum
            ),
            br(),
            conditionalPanel(
              condition = "input.custOpt == 'Grow your own Tree!'",
              # box(
              h4("Minimum Split"),
              helpText(
                "If at a given node N is below this value, that node cannot",
                "be split any further: it is a terminal node of the tree."
              ),
              sliderInput(
                inputId = "minSplit",
                label = NULL, # label given in outer code
                min = 2, # two is the smallest that could be split
                max = 10, # chosen to not make the models too wild
                value = 2 # defaults to not having an artifical minimum
              ),
              br(),
              h4("Minimum Bucket Size"),
              helpText(
                "If creating a given split would cause N₁ or N₂ to fall below",
                "this minimum, then that split isn't made part of the",
                "decision tree."
              ),
              sliderInput(
                inputId = "minBucket",
                label = NULL, # label given in outer code
                min = 1, # can't have buckets of size zero
                max = 30, # rpart default is minbucket = 3*minsplit
                value = 1 # defaults to not having an artifical minimum
              ),
              br(),
              h4("Maximum Tree Depth"),
              helpText(
                "Control the maximum depth that the decision tree can reach.",
                "Note that, depending on what features are being used and the",
                "values of the other parameters, you may end up with a tree",
                "much shallower than the maximum."
              ),
              sliderInput(
                inputId = "maxDepth",
                label = NULL, # label given in outer code
                min = 2, # a min of 2 allows for at least one split
                max = 30, # rpart can't do 31+ depth on 32-bit machines
                value = 5 # chosen to not make the default too wild
              )
              # )
            )
          )
        )
      ),
      tabItem(
        tabName = "randomForest",
        fluidRow(
          box(plotOutput("randomForestPlot", height = 1000))
        )
      ),
      # TODO number of trees; randomize
      ##############################
      # prediction interaction part
      ##############################
      tabItem(
        tabName = "decisionTreePredict",
        fluidRow(
          box(
            fluidRow(
              plotOutput("decisionTreeTrainPlot_", height = 600)
            ),
            fluidRow(
              column(
                6,
                h2("Decision Tree Performance"),
                helpText(
                  "These are the measures of how good your model was",
                  "when it was ran on the test data set. Recall what",
                  "was said in lectures about how we interpret the",
                  "differences between measures these and the measures",
                  "from the training data."
                ),
                # test accuracy, true positive, and true negative
                tagAppendAttributes(
                  textOutput("dt_test_scores_"),
                  # allow linebreaks between scores, larger font here
                  style = "white-space: pre-wrap; font-size: 17px;"
                ),
                br(),
                # training results table matches layout from presentation
                tableOutput("dt_test_table_")
              ),
              column(
                6,
                textOutput("dtPrediction")
              )
            )
          ),
          box(
            h3("Predict which team would win!"),
            span(textOutput("impFeatures"), style="color:#fff"),
            actionButton(
              inputId = "dtModelPredict",
              label = "Predict Winner",
              class = "btn-primary",#"btn-danger" # makes it blue!
              style = "color: #fff"
            ),
            br(),
            br(),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueFirstBlood') > -1",
              radioButtons("blueFirstBlood", "Blue First Blood", c(0, 1))
              # sliderInput("blueFirstBlood", "Blue First Blood", 0, 1, 0, step=1)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueGoldDiff') > -1",
                sliderInput("blueGoldDiff", "Blue Gold Diff", -10000, 10000, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueCSPerMin') > -1",
              sliderInput("blueCSPerMin", "Blue CS Per Min", 0, 40, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueEliteMonsters') > -1",
              radioButtons("blueEliteMonsters", "Blue Elite Monsters", c(0, 1, 2))
              # sliderInput("blueEliteMonsters", "Blue Elite Monsters", 0, 2, 0)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('blueTotalJungleMinionsKilled') > -1",
              sliderInput("blueTotalJungleMinionsKilled", "Blue Total Jungle Minions Killed", 0, 150, 50)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('redTotalJungleMinionsKilled') > -1",
              sliderInput("redTotalJungleMinionsKilled", "Red Total Jungle Minions Killed", 0, 150, 50)
            ),
            conditionalPanel(
              condition = "output.impFeatures.indexOf('redHeralds') > -1",
              radioButtons("redHeralds", "Red Heralds", c(0, 1))
              # sliderInput("redHeralds", "Red Heralds", 0, 1, 0)
            )
                      
            # radioButtons("firstBlood", "First Blood", c("Blue", "Red")),
            # radioButtons("herald", "Herald", c("Blue", "Red")),
            # sliderInput("blueWardsPlaced", "Blue wards placed", 0, 60, 20),
            # sliderInput("blueWardsDestroyed", "Red wards placed", 0, 120, 20),
            # sliderInput("blueELiteMonsters", "Blue Elite Monsters", 0, 2, 1),
            # sliderInput("blueTowersDestroyed", "Blue Towers Destroyed", 0, 1, 1),
            # sliderInput("blueTotalJungleMinionsKilled", "Blue Total Jungle Minions Killed", 0, 80, 20),
            # sliderInput("blueTotalGold", "Blue Total Gold", -10000, 10000, 0),
            # sliderInput("blueMinionKillsPerMin", "Blue Minion Kills Per Min", 10, 30, 20),
            # sliderInput("redWardsPlaced", "Red wards placed", 0, 60, 20),
            # sliderInput("redWardsPlaced", "Red wards placed", 0, 60, 20),
            # sliderInput("redTowersDestroyed", "Red Towers Destroyed", 0, 1, 1),
            # sliderInput("redTotalJungleMinionsKilled", "Red Total Jungle Minions Killed", 0, 80, 20),
            # sliderInput("redTotalGold", "Red Total Gold", -10000, 10000, 0)

          )
        )
      )
      #       tabItem(
      #         tabName = "lexicon",
      #         HTML('<!DOCTYPE html>
      # <html>
      # <head>
      #   <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
      #   <script type="application/shiny-singletons"></script>
      #   <script type="application/html-dependencies">jquery[3.6.0];shiny-css[1.7.1];shiny-javascript[1.7.1]</script>
      #   <head>
      #     <link href="shared/shiny.min.css" rel="stylesheet" />
      #     <script src="shared/shiny.min.js"></script>
      #     </head>
      #     <body>
      #         <h1>Lexicon</h1>
      #         <table>
      #         <tr>
      #             <th>Name</th>
      #             <th>Explanation</th>
      #           </tr>
      #           <tr>
      #             <td>Alfreds Futterkiste</td>
      #             <td>Maria Anders</td>
      #           </tr>
      #         </table>
      #         </body>>
      # </html>')
      #       ),
      # tabItem(
      #   tabName = "introductionVideo",
      #   HTML('<iframe width="560" height="315" src="https://www.youtube.com/embed/0uyLRPmmYPk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>')
      # ),
    ),
    tags$head(tags$style(HTML("
            /* logo */
            .skin-blue .main-header .logo {
                                  background-color: #111111;
                                  };
    #         /* whole page */
    #         .box {margin-top: 2px;margin-left: 0px; margin-right: 0px; margin-bottom:2px;padding:-10px};
    # div {padding: 0 !important;}
                                  ")))
  )


############################################################
### SHINY APP ###
############################################################

ui <- dashboardPage(
  # useShinyjs(),
  # extendShinyjs(text = jsToggleFS),
  headerbar,
  sidebar,
  dashboardContent
)

server <- function(input, output, session) {
  # set.seed(100)
  # histdata <- rnorm(500)
  # output$randomForestPlot <- renderPlot(hist(histdata, plot = FALSE), "plot1")
  # decisionTree <- createTree(train, observe(input$minSplit), observe(input$minBucket), observe(input$maxDepth))
  decisionTree <- eventReactive(

    eventExpr = input$trainDtModel, {
      train.test = train.test.split(lol, input$splitSize/100)
      train = train.test[[1]]
      test = train.test[[2]]
      
      if (input$custOpt == "Optimized Tree") {
        valueExpr = createTree(train)
      }else{
        valueExpr = createTree(train, input$minSplit, input$minBucket, input$maxDepth, optimise = FALSE)
        # print(input$custOpt)
        # print(valueExpr)
      }
    }
  )

  output$decisionTreeTrainPlot_ <- renderPlot(
    rpart.plot(decisionTree(), box.palette = "BuRd", roundint=FALSE)
  )
  output$decisionTreeTrainPlot <- renderPlot(
    rpart.plot(decisionTree(), box.palette = "BuRd", roundint=FALSE)
  )

  output$impFeatures = renderText(get.dt.features(decisionTree()))

  ##############################
  #### eventReactive############
  ##############################

  # regenerate training results every time createModel is pressed
  dt_train_results <- eventReactive(
    eventExpr = input$trainDtModel,
    valueExpr = useTree(decisionTree(), train)
  )
  dt_test_results <- eventReactive(
    eventExpr = input$trainDtModel,
    valueExpr = useTree(decisionTree(), test)
  )

  output$dt_train_scores <- renderText(
    paste(calcScores(dt_train_results()), collapse = "\n")
  )
  output$dt_test_scores <- renderText(
    paste(calcScores(dt_test_results()), collapse = "\n")
  )
  output$dt_train_table <- renderTable(
    resultsTable(dt_train_results()),
    align = "lccc", # left-align first column, centre rest
    striped = TRUE
  )
  output$dt_test_table <- renderTable(
    resultsTable(dt_test_results()),
    align = "lccc", # left-align first column, centre rest
    striped = TRUE
  )

  output$dtPrediction <- eventReactive(
    eventExpr = input$dtModelPredict, {
      valueExpr = predict.winner(decisionTree(),       
                                 data.frame(blueFirstBlood = c(as.integer(input$blueFirstBlood)),
                                            blueEliteMonsters = c(as.integer(input$blueEliteMonsters)),
                                            blueTotalJungleMinionsKilled = c(input$blueTotalJungleMinionsKilled),
                                            blueGoldDiff = c(input$blueGoldDiff),
                                            blueCSPerMin = c(input$blueCSPerMin),
                                            redHeralds = c(as.integer(input$redHeralds)),
                                            redTotalJungleMinionsKilled = c(input$redTotalJungleMinionsKilled)),
                                 model.type = 'dt'
                                 )

      # print(valueExpr)
    }
  )

  output$dt_test_scores_ <- renderText(
    paste(calcScores(dt_test_results()), collapse = "\n")
  )

  output$dt_test_table_ <- renderTable(
    resultsTable(dt_test_results()),
    align = "lccc", # left-align first column, centre rest
    striped = TRUE
  ) 

  ##############################
  #### observeEvent############
  ##############################

  # observeEvent(input$addLine, {
  #   new_id <- paste("row", input$addLine, sep = "_")
  #   insertUI(
  #     selector = "#placeholder",
  #     where = "beforeBegin",
  #     ui = sliderInput("blueWardsDestroyed", "Red wards placed", 0, 120, 20))}
  #   )

  # # obvervation for customizer and optimizer
  # observeEvent(input$custOpt, {
  #   if (input$custOpt == 'Customizer'){
  #     shinyjs::show(id = "customizer-optimizer")
  #   }else {
  #     shinyjs::hidden(id = "customizer-optimizer")
  #   }
  # })


  # height = 1000,
  # width = 1000,
  # xlab = "",
  # ylab = "",
  # main = "Decision Tree"
}

shinyApp(ui, server)