library(caret)
library(tree)
library(randomForest)
library(dplyr)
library(rattle)
library(corrplot)
library(rpart)

# read data
setwd("~/OneDrive/Documents/MyOversea/Cass Study/machine learning/MTP/MTP1/machine-learning-gp1")
lol.ori = read.csv("high_diamond_ranked_10min.csv", header = TRUE)

# summary of data
str(lol.ori)
summary(lol.ori)

# pre-process data
sum(is.na(lol.ori)) # check missing values
lol.ori = lol.ori[,-1]

set.seed(100)
lol.blue = lol.ori[sample(which(lol.ori$blueWins == 1,), 240),]
lol.red = lol.ori[sample(which(lol.ori$blueWins == 0,), 240),]
lol.sample = rbind(lol.blue, lol.red)

# feature engineering
par(mfrow=c(1,1))
blue.features = lol.sample[,c(2:20)]
corrplot(cor(blue.features), tl.col = "black", diag = FALSE)
drop.blue.features = c('blueDragons', 'blueHeralds', 'blueKills', 
                       'blueDeaths', 'blueAssists', 'blueTotalGold',
                       'blueAvgLevel', 'blueTotalExperience', 'blueExperienceDiff',
                       'blueGoldPerMin', 'blueTotalMinionsKilled')
blue.features = blue.features[, !(colnames(blue.features) %in% drop.blue.features)]

red.features = lol.sample[,-c(1, 2:20)]
corrplot(cor(blue.features, red.features), tl.col = "black", diag = TRUE)
drop.red.features = c('redFirstBlood','redKills', 'redDeaths', 'redAssists', 
                      'redEliteMonsters', 'redDragons', 'redTotalGold', 
                      'redAvgLevel', 'redTotalExperience', 'redTotalMinionsKilled',
                      'redGoldDiff', 'redExperienceDiff', 'redCSPerMin', 'redGoldPerMin')
red.features = red.features[, !(colnames(red.features) %in% drop.red.features)]
blueWins = lol.sample$blueWins

lol = cbind(blue.features, red.features, blueWins)
corrplot(cor(lol), tl.col = "black", diag = FALSE)
drop.more.features = c('blueWardsPlaced', 'blueWardsDestroyed', 'blueTowersDestroyed',
                       'redWardsPlaced', 'redWardsDestroyed', 'redTowersDestroyed')
lol = lol[, !(colnames(lol) %in% drop.more.features)]

# modify classification column
lol$blueWins[lol$blueWins == 1] <- "Blue"
lol$blueWins[lol$blueWins == 0] <- "Red"
lol$blueWins = factor(lol$blueWins)

set.seed(100)
# random split to training and test set
# train.index = createDataPartition(lol$blueWins, p = 0.6, list = FALSE)
train.index = createDataPartition(lol$blueWins, p = 0.7, list = FALSE)
train = lol[train.index,]
test = lol[-train.index,]


createTree <- function(train_data, min_split, min_bucket, max_depth) {
    # Takes a list of model variables (strings), a minimum split parameter
    # (int), a minimum bucket size parameter (int), and a maximum tree depth
    # parameter (int) as inputs and then returns an rpart classification tree
    # created with those parameters using Gini-indexes without any pruning.
    # create an rpart compatible formula for the model from the chosen vars
    # f <- paste("blueWins ~ ", paste(model_vars, collapse = " + "))
    # rpart is performs the split calculations and returns the tree
    tree <- rpart(
        # as.formula(f),
        "blueWins ~ .",
        method = "class", # sets it up as a classification problem
        data = train_data,
        parms = list(split = "gini"), # ensures rpart uses gini indexes
        minsplit = min_split,
        minbucket = min_bucket,
        maxdepth = max_depth,
        cp = 0 # complexity parameter, at zero prevents pruning on branches
    )

    return(tree)
}


evaluteTree <- function(tree) {
    # Takes a tree generated by rpart and a filename (string) as input and
    # then predicts the labels of the data in that file using the tree. It
    # returns a dataframe with two bool (0,1) columns: prediction and truth.
    trainX = train[,-(length(train)+1)]
    trainY = train[,length(train)]
    testX = test[,-(length(test)+1)]
    testY = test[,length(test)]
    pred.y.Train <- predict(tree, trainX, type = "class")
    pred.y.Test <- predict(tree, testX, type = "class")
    accuracyTrain = mean(pred.y.Train == trainY)
    accuracyTest = mean(pred.y.Test == testY)
    return(list(train.accuracy = accuracyTrain, test.accuracy = accuracyTest))
}